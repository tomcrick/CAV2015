\documentclass{llncs}
%
\usepackage{makeidx}  % allows for indexgeneration
\usepackage[british]{babel}
\usepackage{url}
\usepackage[pdftex,colorlinks=true]{hyperref}
\usepackage{graphicx}
%\fussy


\title{Dear CAV, We Need to Talk About Reproducibility}

\author{Tom Crick\inst{1} \and Benjamin A. Hall\inst{2} \and Samin Ishtiaq\inst{3}}

\institute{Department of Computing \& Information Systems\\Cardiff Metropolitan University, UK\\
\email{tcrick@cardiffmet.ac.uk}
\and
MRC Cancer Unit, University of Cambridge, UK\\
\email{bh418@mrc-cu.cam.ac.uk}
\and
Microsoft Research Cambridge, UK\\
\email{samin.ishtiaq@microsoft.com}
}

\raggedbottom
\begin{document}
%
\frontmatter          % for the preliminaries
%
\pagestyle{headings}  % switches on printing of running heads
%\addtocmark{} % additional mark in the TOC

\maketitle

\begin{abstract}
How many times have you tried to re-implement a past CAV tool paper,
and failed? 
Reliably reproducing published scientific discoveries has been
acknowledged as a barrier to scientific progress for some time but
there remains only a small subset of software available to support the
specific needs of the research community (i.e. beyond generic tools
such as source code repositories). In this paper we propose an
infrastructure for enabling reproducibility in our community, by
automating the build, unit testing and benchmarking of research
software. 
\end{abstract}

% Keywords for Easychair:
% Reproducibility
% Artifact evaluation
% Benchmarks
% Verification
% Validation

% TO DO:
% consistency of artefact (UK) vs. artifact (US)...
% missing refs
% trim to six pages!

\section{Introduction}\label{intro}

%% \begin{quotation}
%% {\emph{CAV 2015 is the 27th in a series dedicated to the advancement
%%     of the theory and practice of computer-aided formal analysis
%%     methods for hardware and software systems.  CAV considers it vital
%%     to continue spurring advances in hardware and software
%%     verification while expanding to new domains such as biological
%%     systems and computer security. The conference covers the spectrum
%%     from theoretical results to concrete applications, with an
%%     emphasis on practical verification tools and the algorithms and
%%     techniques that are needed for their implementation.}}
%% \end{quotation}
 
This is not a theory paper or a tool paper, nor an industrial case
study. The aim of this paper is to start a discussion in the CAV
community about the reproducibility of algorithms, models, tools and
benchmarks across the computer-aided formal analysis and verification
research domain. There is a significant opportunity for the CAV
community to identify and address the technical and socio-cultural
issues surrounding reproducibility in both the cognate research domain
as well as more broadly for computing science; a desirable outcome
would be a clear specification to encourage, enable and enforce
reproducibility. Expressing a standard for reproducibility would have
a clear benefit for researchers as well as the CAV community as a
whole.

We do not need to sell the idea of reproducibility to the CAV
community too much. We are used to writing papers about our algorithms
and tools.  And reproducing others' work, or having our work
reproduced, is the guts of those ``Benchmark Tables'' that CAV authors
and referees both think are essential to their papers. In fact, the
idea of reproducibility is gaining momentum in the wider scientific
community of computer science~\cite{collberg-et-al:2014}, life
sciences~\cite{rollins-et-al:2014},
psychology~\cite{chambers-et-al:2014} and the social
sciences~\cite{conte-et-al:2012}.
There has been a revolution in the sharing and dissemination of published
papers (\emph{open access}) and the subsequent discussions relating to
the sharing of protocols and materials (\emph{open science}). But the 
ability of a researcher to take published results and data and
reimplement the described workflow remains difficult~\cite{stodden-et-al:2013,sandve-et-al:2013,wilson-et-al:2014},
\footnote{Retraction Watch: Tracking retractions as a window
into the scientific
process\\\url{http://retractionwatch.com/}}. 
We have previously documented some of
the technical and cultural barriers to reproducing work across
computing and the computational sciences, both in terms of the sharing
of algorithms~\cite{crick-et-al_recomp2014} and models and benchmark
sets~\cite{crick-et-al_wssspe2}.

Quite a few computer science conferences ACM
SIGMOD\footnote{\url{http://db-reproducibility.seas.harvard.edu/}},
CGO\footnote{\url{http://ctuning.org/cm/wiki/index.php?title=Reproducibility:AE:CGO2015}},
SPLASH
(OOPSLA)\footnote{\url{http://2014.splashcon.org/track/splash2014-artifacts}},
PPoPP\footnote{\url{http://ctuning.org/cm/wiki/index.php?title=Reproducibility:AE:PPoPP2015}},
TRUST\footnote{\url{http://ctuning.org/cm/wiki/index.php?title=Events:TRUST2014}}
(PLDI) and ADAPT\footnote{\url{http://www.adapt-workshop.org/}}
(HiPEAC) now explicitly acknowledge the importance of reproducibility
(and repeatability, recomputability and the multitude of `Rs' that
underpin e-research).  For many --- like CAV this year --- this takes
the form of the author providing some \emph{artefacts} (an accessible
tool for reproducing results) to evaluate. Journals like Nature, PLoS
Computational Biology and Bioinformatics explicitly require that
source code and data is made available online under some form of open
source license. While these iniative are great, they are often
optional, seem piecemeal, and do little to enable verification or
validation of scientific results at a later stage. Even within the
same field, there are different ideas of what the definition of
reproducibility is. Some open discussion and understanding of what is
standard would be good for the commmunity: we need to explicitly state
that this is important and address it, or don't bother doing it at all.

This paper is a ``Call to Action'', inviting CAV practitioners to
embrace a new methodology. Practically, we propose an initial
specification specification for a reproducibilty service for CAV. We
present the requirements of the prototype, and a suggested plan for
introducing the tool to the community. We have a running example based
upon a previously accepted CAV paper by the authors. Finally, we
highlight key implementation issues relating to security and general
applicability which will need mitigating or resolving before
widespread acceptance by the research community.

\section{A specification for reproducible computational science}\label{spec}

A service for reproducibility is intended to play three important roles. It should:
\begin{enumerate}
	\item Demonstrate that a piece of code can be compiled, run and behaves as described,
		without manual intervention from the developer.
	\item Store and link specific artefacts and their with different publications
		or other publicly-accessible datasets.
	\item Allow new benchmarks to be added, by users other than the developer, to 
		widen the testing and identify potential bugs.
\end{enumerate}
 
Furthermore, we feel that such a service must require minimal
developer intervention.  This serves multiple purposes -- through
automation for example, the service can be enabled to compile new code
and test new benchmarks trivially. This also forces the developer to
make local workarounds or hacks publicly viewable. As such, this
requires the developer to make the project dependencies clearly
available, and enables future changes in the dependencies (such as a
library update) to be tested automatically too.

Finally, the service must fit easily into the developers workflow. As
noted in section~\ref{rollout} we expect that there will be some costs
to the users in terms of the time required to ensure that the code
compiles and runs on the service. To minimise this, the service needs
to connect to standard code repositories, automatically detecting and
responding to new versions of the code and updates to dependencies,
running tests for every new code commit.

\begin{figure}[!ht]
	\centering
	\includegraphics[width=\textwidth]{workflow}
	\caption{Proposed reproducibility service workflow}
	\label{schematic}
\end{figure}
	
To address these needs we propose that the service should follow the
scheme given in Figure~\ref{schematic}. Two classes of user are
defined; developers, who generate code, and modellers, who generate
new benchmarks. An individual might in practice play either or both of
these roles; here the roles serve to define ways in which people
interact with the system. A developer writes new code, which is
periodically pushed to a repository such as GitHub. Through
integration with the repository, the server responds to new code by
undergoing a process of pulling the code from the repository,
downloading required dependencies, and compiling the code. If this
stage fails, the developer is informed and the workflow ends. If the
code is successfully compiled, two stages of testing are
performed. The first stage (labelled {\emph{test}} in
Figure~\ref{schematic}), involves running a series of basic tests
defined by the developer. This is intended as a sanity check to ensure
that basic features of the code have not been broken by the updated
code, and failure to pass these tests is reported and ends the work
flow. If this completes successfully, the second stage (labelled
{\emph{benchmark}} in Figure~\ref{schematic}) a series of models are
tested for a known property, and the results recorded. These results
can then be stored in a database, with a note of the commit ID, and
available through a web interface for future analysis.

In contrast to the developer role, a modeller supplies benchmarks for
a piece of code to test. These do not require that the latest version
of the code is recompiled, but on submission the models are tested and
added to the local repository of models for analysis. After a model is
submitted, the model is tested on every new piece of code pushed to
the server and the changes in the behaviour can be noted and linked to
specific code commits. Whilst the developers role has a transparent
value (in providing an implementation of an algorithm), the value of
the modeller may be less immediately clear. The modeller submits a
broad range of tests which may highlight material flaws (i.e. bugs) in
the implementation, or the algorithm. More than this however, the
modeller may generate models which identify weaknesses of either an
algorithm or an implementation. One example from the authors
experience is the series of models with ``timed-switches'' described
in \cite{cook2014}; this is discussed in greater depth below in
section~\ref{example}.

Finally, dependencies for a given implementation need explicit
testing. Due to the highly variable and sometimes complex nature of
dependencies, we see this as an optional part of the workflow, as
developers may chose to supply dependencies as binary files in the
code compilation process. For completeness however we note that such a
system could also respond to updates in external dependencies by
triggering compilation and testing in the same manner as defined for a
new code commit. This would aid developers in identifying code
breaking changes induced by third parties.


%\section{Introducing reproducibility testing to the community}\label{rollout}
\section{A reproducibility model for the CAV community}\label{rollout}

Following the proposal of such a system, the question becomes:
{\emph{how do we encourage widespread uptake, or even standardisation?}}
Such a service may appear non-trivial, given the large numbers of
tools and workflows
that could potentially require to be supported by the service. Furthermore,
after such a service has been implemented, how do we ensure it is
\emph{useful} and \emph{usable} for researchers. To address this, we
propose the following workflow for CAV:

% workflow
\begin{enumerate}
\item Before conference: clear signposting for authors; it should be
advertised and promoted in the CAV call for papers to highlight this
is a step-change in how we address reproducibility. Call for artefact
reviewers with a range of specialisms, with a named chair of the review team.
\item Explicit criteria for authors? Essentially: {\emph{make this as
easy as possible for us to evaluate/execute your artefact!}}. We would
aim to articulate the review criteria, but the primary aim is: {\emph{can I evaluate/execute this
  artefact and get the same results that are presented in the paper?}}
\item Reviewing: in the first instance, it may be seen as an extra
(voluntary) step to the normal reviewing process: e.g. {\emph{This
submission is voluntary and will not influence the final decision
regarding the papers.}}. Independent of the scientific merit of the
paper, the results will be verified. To encourage this, there may be a
prize, as well as ranked ordering and listed in conference proceedings
(seal of approval?).
\item Submission: when papers are submitted, they have to nominate whether they
  want their paper to go through artefact review (at the start, this
  may not be compulsory, but this will change over a period of
time -- effecting cultural change and this would then become a
necessary condition and a formal stage in the reviewing workflow),
  along with required tools, libraries and (ideally) computational requirements.
\item Evaluation: artefact evaluation process runs concurrent to the
  standard paper review process.
\item Reporting: traffic lights system (ranked list?) to indicate the level of
  reproducibility of the submitted artefact. 
\item Community curation: over a number of CAV cycles, we would have a
  community curated repository/database of previous artefacts, which
  would provide exemplars, comparisons and emerging best practice.
\end{enumerate}

The key question for different research communities then becomes: how
to initialise this change? Such a requirement creates a set of new
costs to researchers, both in terms of time spent ensuring that their
tools work on the centralised system (in addition to their local
implementation), but also potentially in terms of equipment (in terms
of running the system). Such costs may be easier to bear for some
groups compared to others, especially those with large groups who can
more easily distribute the tasks, and it is important that the service
does not present a barrier to early career researchers and those with
efficient budgets. This type of cost is not unique to reproducibility
efforts- it has been estimated (in CACM, a closed but arguably
affordable journal) that a shift to becoming exclusively open access
may lead to a ten-fold increase in computer science publication
costs~\cite{vardi-cacm-2014}. 

\section{Example case study: BioModelAnalyzer}\label{example}

% NEEDS WORK!
% IDEA: pick one of Samin's CAV papers (BMA?) and run it through the
% process and see what happens.

The
BioModelAnalyzer\footnote{\url{http://biomodelanalyzer.research.microsoft.com/}}
(BMA) is a tool for the development and analysis of a specific class of
formal models for
biology~\cite{benque2012,cook-et-al:2010,cook2014}. Models are built
by biologists in a web browser and are explored at present by a
combination of simulation and model checking. The tool specifically
allows users to test for model \emph{stability}; that is, a bespoke
algorithm proves that for all initial states a model always ends in a
single fixpoint. We have chosen this example due to our familiarity
with the tool, and to highlight historical examples where a
reproducibility service would have supported both code development and
algorithm discovery.

The benefits for reproducibility testing for a service are clear. In a sense,
reproducibility here is an extension of the standard practice of \emph{testing}.
Three features distinguish our aims for reproducibility here: compilation and testing
in a new machine, a continuous integration strategy for code commits, and 
the ability to add and remove benchmarks to the test set.

\subsection{\emph{de novo} Build Environments}

Throughout the lifetime of the BMA development has been shared between a 
number of developers, working on different aspects of the tool. Work in 
algorithm development~\cite{cook2014,piterman2013} focuses on adding 
new features to a command line tool with few dependencies, aiding rapid 
development. In contrast, the graphical model construction and testing 
environment has typically been done by a single or pair of individuals. 
This necessarily required a number of dependencies, reflecting the use
of Azure and Silverlight.

In an early stage of development it was found that only a single machine 
was capable of deploying the web service. This arose as the developer 
responsible for writing and deploying the user interface had run a series
of commands necessary to run the mixture of 64- and 32- bit components on
Azure. These commands needed only be run once, and went undocumented, and 
needed to be rediscovered later when other team members attemped to deploy.
These problems would be identified trivially through the proposed service;
such undocumented commands would lead to all tests failing until added to 
the build process.

\subsection{Tool Refinement}

Throughout the development of the tool, many refinements have been made 
to different implementations. Some of these were subtle, and were  
identified by unit tests; for example rounding mechanisms were switched
between floors and rounds following a scientific discussion. More complex
changes however broke behaviours which were not tested in our available 
benchmark set. One example was in the treatments with of nodes without
inputs- ``biological intuition'' suggested that such nodes should have
an alternative default function from other nodes. Here, the ability of 
users to submit new benchmarks would aid identification of these breaking
changes, by extending the test sets and simplifying the process of adding
to the test sets, and forcing the question of what changes are appropriate
(and how to update old models to keep correct behaviour).

%skip this section? It just repeats the same problem as above
%Modelling reveals untested issues: range conversion
%(no original benchmarks had range conversion- new benchmarks did
%and showed unexpected behaviour, including a mismatch between symbolic
%and explicit implementations of range conversion)
%Service would- allow external users to add new benchmarks, which would make it
%easier to catch these issues

\subsection{Identification of Algorithmic Weaknesses}

In~\cite{cook2014} we presented a new algorithm for proving stability in 
a new class of models. Whilst the paper focused on discussing the algorithm,
identifying the new class of models was complex. Models with long cycles 
and the new class of models (``non-trivially stable models'') both can take
substantial time to search for cycles, and these models could only be proved
stable using a combination of simulation (to identify the fix-point) and LTL
queries (to prove that there existed no paths beyond a certain length which 
did not include the fix-point)~\cite{piterman2013}. 

The proposed service would allow both types of test to be included explicitly,
and models to be routinely added to each algorithm. Models which time-out with 
one but are successfully proved can be logged and identified for future study.
The features which define them would be more easily found, and new algorithms
developed to address the specific features of the model. It could further be 
used to demonstrate the speed improvement arising from new algorithms.

% Worked example of how issues could be highlighted and found over time: perhaps
% another figure? linking code changes to discoveries could be drawn as a timeline. The 
% proposed output of the hypothetical service could be indicated alongside it, 
% demonstrating how the service would have supported the work.

% should this be merged into the section above? Cite Fursin et al.
% Problem: getting people to do it? Carrot/stick...
\section{Known unknowns: issues for a reproducibility service}\label{issues} 

How to effectively ``generalise'' for reuse and then cascading to other communities?

How best to secure a service which compiles and runs arbitrary code on arbitrary inputs?

How to estimate raw performance on the cloud?

\section{Conclusions}\label{concl}
We thus propose that this could be trialled for CAV in 2016, with a
announced schedule for widespread adoption in future years...

The benefits to the community from a cultural change to favour
reproducibility are clear however, and as such we should aim through
the software and the roll-out to mitigate these costs. Furthermore, we
can reasonably expect the needs of the community to evolve over time,
and initial implementations of the platform may require refinement in
response to user feedback. As such, if the community is to move to
requiring reproducibility, it seems most reasonable that this is
staggered over a number of years to allow for both of these elements
to develop. We recommend that the tools pass through two testing
phases, until eventually all authors are required to use the service.

\begin{description}
\item[Phase 1:] Offer the service as an optional extra in the testing phase, allowing users to demonstrate 
the reliability of their code which could be taken into account in the review process.
\item[Phase 2:] All authors must use the reproducibility service, but results are not used in the review
process. The results of the test are used to refine the service and pick out any unaddressed issues
\item[Phase 3:] All authors are required to use the service, and the results are explicitly used to 
assess reproducibility in the review process.
\end{description}

This plan balances competing needs within the community, and would reduce the activation
energy for uptake by gradually introducing it to authors.

\bibliographystyle{splncs}
\bibliography{cav2015}

\end{document}
