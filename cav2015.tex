\documentclass{llncs}
%
\usepackage{makeidx}  % allows for indexgeneration
\usepackage[british]{babel}
\usepackage{url}
\usepackage[pdftex,colorlinks=true]{hyperref}
%\fussy


\title{A Model of Reproducibility for CAV}

\author{Tom Crick\inst{1} \and Samin Ishtiaq\inst{2} \and Benjamin A. Hall\inst{3}}

\institute{Department of Computing \& Information Systems\\Cardiff Metropolitan University, UK\\
\email{tcrick@cardiffmet.ac.uk}
\and
Microsoft Research Cambridge, UK\\
\email{samin.ishtiaq@microsoft.com}
\and
MRC Cancer Unit, University of Cambridge, UK\\
\email{bh418@mrc-cu.cam.ac.uk}
}

\raggedbottom
\begin{document}
%
\frontmatter          % for the preliminaries
%
\pagestyle{headings}  % switches on printing of running heads
%\addtocmark{} % additional mark in the TOC

\maketitle

\begin{abstract}
Reliably reproducing published scientific discoveries has been acknowledged as a barrier to 
scientific progress. Even when pubhlished results reflect the authors interpretation perfectly,
novel implementation of published algorithms, and their subsequent benchmarking leaves 
substantial room for error. However, whilst this phenomena has been recognised and documented for some
time, there remains only a small subset of software available to support the specific 
needs of the research community (i.e. beyond generic tools such as git). Here we propose a 
concrete platform for reproducibility, based on a prototype which supports previously published 
work by some of the authors. The aim of this platform is to automate the build, unit testing,
and benchmarking of the BioModelAnalyzer software. We propose this prototype as a future
model for promoting and embedding reproducibility into CAV.

\end{abstract}

\section{Introduction}\label{intro}
The reproducibility of scientific discovery has come into sharp focus over the past few years.
Following a revoltion in the sharing and dissemination of published papers (\emph{Open Access})
and the subsequent discussions relating to the sharing of protocols and materials (\emph{Open
Science}) the ability of a researcher to take published data and reimplement the described
workflow has been under scrutiny~\cite{gent:2013,sandve-et-al:2013}. This has been further supported by wider cultural issues in
scientific community, where irreproducible, and frequently high profile results 
are increasingly being identified and retracted from the literature~\cite{retraction_watch}.
We have previously documented some of the technical and cultural barriers to reproducing 
work across multiple disciplines in the computational sciences, both in terms of the sharing
of algorithms~\cite{crick-et-al_recomp2014} and benchmark sets~\cite{crick-et-al_wssspe2}. 
The impact of irreproducibility is however broadly felt across multiple disciplines, with 
well discussed issues psychology~\cite{chambers-et-al:2014} and others.

Reproducibility has now been explicitly acknowledged as a desirable quality in a number of 
different conferences, including CAV (through the CAV Artefact Evaluation) and TACAS. Some conferences explcitly note that 
the provision of \emph{artefacts} (an accessible tool for reproducing results) is desirable
but does not change the ultimate outcome of the review e.g. OOPSLA. Several biological journals
(e.g. Bioinformatics, PLoS Computational Biology) explicitly require that source code is
made available under an open source license, or through a web interface. 

%Cite: Recomputation Manifesto~\cite{gent:2013}, 10 Simple Rules for Reproducible
%Research~\cite{sandve-et-al:2013}, Stodden~\cite{stodden-et-al:2013},
%changes to dissemination in other fields e.g. psychology~\cite{chambers-et-al:2014}.

% http://2014.splashcon.org/track/splash2014-artifacts#About
From SPLASH: {\emph{OOPSLA Artifacts: The Artifact Evaluation Committee has been formed
to assess how well paper authors prepare artifacts in support of such
future researchers. Roughly, authors of papers who wish to participate
are invited to submit an artifact that supports the conclusions of the
paper. The Artifact Evaluation Committee will read the paper and
explore the artifact to give the authors third-party feedback about
how well the artifact supports the paper and how easy it is, in the
committeeâ€™s opinion, for future researchers to use the artifact. This
submission is voluntary and will not influence the final decision
regarding the papers. Papers that go through the Artifact Evaluation
process successfully receive a seal of approval printed on the first
page of the paper in the OOPSLA proceedings.}}

Other examples: ACM TRUST~\cite{fursin+dubach:2014}; using proofs and
Coq e.g. POPL/PLDI/ICFP.

% Q: do we need to define artefact? different from model, etc, also
% domain terminology e.g. CS vs. physics.

However, whilst provision of a single implementation or the current source
code may not be sufficient to reproduce computational discoveries satisfactorily.
As discussed previously, external dependencies may change the behaviour, or 
in the case of an incompatible upgrade, prevent compilation. Other factors include
... These points of failure could be partially addressed by going beyond the sharing
of source code and benchmarks, and sharing complete systems which build code and 
test the software in question. Such a system could be run centrally, independent of 
the researchers, and new papers could be associated with a submission of code and
a set of benchmarks. This would ensure that the code compiles and runs as described,
but would also serve as a repository of artefacts, allowing future researchers,
who are perhaps suffering under a burden of code which does not compile or models
which do not behave, a trusted example of a working implementation.

Following the proposal of such a system, the question becomes: how do we achieve 
this? Such a service may appear trivial, given the large numbers of tools which
could potentially support the service. There remain however key questions that
can only be answered on implementation. Furthermore, after such a service has been
implemented, how do we ensure it is \emph{useful} and \emph{usable} for researchers.

The key question for different research communities then becomes: how to initialise 
this change? Such a requirement creates a set of new costs to researchers, both in 
terms of time spent ensuring that their tools work on the centralised system (in 
addition to their local implementation), but also potentially in terms of equipment
(in terms of running the system). Such costs may be easier to bear for some groups 
compared to others, especially those with large groups who can more easily distribute 
the tasks, and it is important that the service does not present a barrier to early 
career researchers and those with efficient budgets. This type of cost is not unique 
to reproducibility efforts- it has been estimated (in CACM, a closed but arguably 
affordable journal) that a shift to becoming exclusively open access may lead to a 
ten-fold increase in computer science publication costs~\cite{vardi-acm-2014}.

Year 0: you can offer it as an optional extra, or...do it for everyone
and then present the results next year to show the issues
(carrot/stick?). There is a genuine time/research cost? Yes, but this
is the key: it has to be done and it thus part of effecting cultural
change within a community...


{\textbf{IDEA: pick one of Samin's CAV papers (BMA?) and run it through the process
and see what happens.}}


\begin{enumerate}
\item Call for papers: clearly advertised and high profile in conf cfp
  -- this is a new thing and a change in how we're doing stuff.
\item The game is changing, but this is (currently) extra to the
  normal reviewing process: 
e.g. {\emph{This submission is voluntary and will not influence the final decision
regarding the papers.}} -- independent of the scientific merit of the
paper, the results will be verified 
\item (prize? as well as ranked ordering at end of conf, listed in
  proceedings, badge, seal of approval, etc)
\item explicit criteria for authors? but essentially: {\emph{make this
      as easy as possible for us to evaluate/execute your artefact!}}
\item when papers are submitted, they have to nominate whether they
  want their paper to go through artefact review
\item (then you need artefact reviewers, probably taken from the pool of
  reviewers, but will need specialism)
\item specify review criteria, but essentially: {\emph{can I evaluate/execute this
  artefact and get the same results that are presented in the paper?}}
\item reporting: traffic lights and ranked list
\item at the start, this is not complusory, but this will change over a period of
time -- effecting cultural change and this would then become a
necessary condition.
\item Repo/database or previous artefacts, which would build over the
  years to give exemplars and comparisons.
\end{enumerate}

How to effectively ``generalise'' for reuse and then cascading to other communities?

\section{Conclusions}\label{concl}
We've done this for CAV...



\bibliographystyle{splncs}
\bibliography{cav2015}

\end{document}
